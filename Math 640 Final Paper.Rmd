---
title: "Math 640 Final Paper"
author: "Max Kearns and Tommy Jones"
date: "May 10, 2018"
output:
  pdf_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# load libraries
library(textmineR)
library(gtools)
library(mcmcplots)
library(EnvStats)

```

\newpage
# Introduction

The analysis of text data is an area of vital research in both frequentist and Bayesian statistics. Text can, and indeed does, store a vast amount of information that is not easily evaluated with well-understood statistical methods. While text analysis is used throughout our economy, it does not have nearly as much research and knowledge behind it as does numerical data. This paper attempts to slightly further the bank of techniques for text analysis, in the hopes that text data will someday be as understood as numerical data is today. 

One method that researchers currently use to model text frequencies is with a Dirichlet prior on a multinomial likelihood. The prior provides uncertainty on $\theta$, which is the vector of word probabilities. The usual model then fixes the Dirichlet parameter ($\alpha$). In a Bayesian setting, however, this approach seems overly simplistic, and MCMC methods provide a simple solution to sample from a more complex distribution. This research intends to start to answer the question as to whether more uncertainty on $\alpha$ would improve the model. Zipf's law provides a basis for how to vary $\alpha$ in a way that is consistent with knowledge about human language. 

Zipf's law is an empirical property of natural language. It states that the word frequencies of any corpus of text follows a power law distribution, regardless of context or language. This means that the most common word will be twice as frequent as the second most common word, and $n$ times as frequent as the $n$th most common word. (citations needed) Based on what Zipf's law dictates, this research tests the viability of placing a Pareto($\gamma$, $\beta$) prior on $\alpha$, where $\gamma$ is fixed at a sufficiently small value. This Pareto distribution is a power-law that should provide some uncertainty on $\alpha$ that mirrors this inherent property of language.  

In order to determine whether this prior merits further research on more complex models, we will begin by modeling a small data set using a simple model that features a multinomial likelihood, a Dirichlet prior, a pareto hyper-prior, with a non-informative Jeffrey's hyper-prior . We will compare this model to a control that does not allow for uncertainty on $\alpha$ The data set is 100 randomly sampled NIH grant proposals from 2014.

# Methods 

## Data 

The data used in this analysis was 

## Models

For both the control and the new model, we assume that the word count $y$ is iid multinomial, so has the following likelihood. (Is there a reason we aren't using vector notation here)

\begin{align}
y \sim multinom(n,\theta) \implies \mathcal{L}(y|\theta,\alpha,\beta) \propto \prod_k \theta_k^{y_k}
\end{align}

On $\theta$, the vector of word probabilities, we place a Dirichlet prior.

\begin{align}
\theta \sim Dir(\vec{\alpha}) \implies \pi(\theta) = \mathcal{B}(\vec{\alpha})\prod_k \theta_k^{\alpha_k - 1}
\end{align}

The control model will stop here, and assume (insert discussion of chosen alpha here). The control model then has the simple form $\theta|y \sim Dir(\vec{y}+\vec{\alpha})$  The candidate model, however, will assume a $Pareto(1, \beta$) prior on $\alpha$, with Jeffrey's prior on $\beta$.

\begin{align}
\alpha_k \sim Pareto(\gamma, \beta) \implies \pi(\vec{\alpha}) &= \prod_k \gamma^\beta\beta\alpha_k^{-(\beta + 1)}\\
\pi(\beta) &\propto \frac{1}{\beta}
\end{align}

This results in the unknown posterior below, and a full derivation of the model can be found in Appendix B. 

\begin{align}
  P(\theta,\alpha,\beta|y) 
    &\propto\beta^{(K-1)}\gamma^{k\beta}\mathcal{B}(\vec{\alpha})
      \prod_k\theta_k^{y_k+\alpha_k - 1}\alpha_k^{-(\beta + 1)}
\end{align}

This prior is unrecognizable, so we will proceed be making a Gibbs sampler of the full posteriors, which can be found below. 

\begin{align}
  P(\theta|\alpha,\beta,y)
    &\propto \prod_k \theta_k^{y_k+\alpha_k - 1}\\
  P(\alpha|\theta,\beta,y) 
    &\propto \mathcal{B}(\vec{\alpha})
      \prod_k \theta_k^{y_k+\alpha_k - 1}\alpha_k^{-(\beta + 1)}\\
  P(\beta|\theta,\alpha,y)
    &\propto \beta^{K-1} \exp\Bigg[-\beta \bigg(\sum_k\log(\alpha_k)-k\log(\gamma)\bigg)\Bigg]\\
\end{align}

Of these conditional posteriors, only $\alpha|\theta,\beta,y$ is an unknown distribution. $\theta|\alpha,\beta,y$ is a $Dir(\vec{y}+\vec{\alpha})$, and $\beta|\theta,\alpha,y$ is a Gamma$\bigg(k, \sum_k\log(\alpha_k)-k\log(\gamma)\bigg)$. The other two conditionals will be sampled from using a Metropolis-Hastings algorithm with (insert proposals).

##Sampling

### Description of data

```{r, echo = FALSE}

# load and prepare the data
dtm <- CreateDtm(doc_vec = nih_sample$ABSTRACT_TEXT, # nih_sample from textmineR
                 doc_names = nih_sample$APPLICATION_ID,
                 ngram_window = c(1,1),
                 stopword_vec = c(),
                 verbose = FALSE)

y <- colSums(dtm) # nih_sample_dtm from the textmineR package

y <- y[ order(y, decreasing = TRUE) ]


```


### Sampling the control model

```{r}
# load the sampling function (to be copied here later)

```

### Sampling the main model

```{r}
# load the sampling function (to be copied here later)

```


##Comparison

# Results 

# Conclusion


# References
##make sure to cite the data source
##Cite info on Zipf's law


# Appendix A

# Appendix B

## Posterior Derivations

\begin{align}
  P(\theta,\alpha,\beta|y) 
    &\propto \left[\prod_k \theta_k^{y_k}\right] 
      \left[\mathcal{B}(\vec{\alpha})\prod_k \theta_k^{\alpha_k - 1}\right]
      \left[\prod_k \gamma^\beta\beta\alpha_k^{-(\beta + 1)}\right]\\
    &= \beta^{K-1}\gamma^{\beta k}\mathcal{B}(\vec{\alpha})
      \prod_k\theta_k^{y_k+\alpha_k - 1}\alpha_k^{-(\beta + 1)}\\
  P(\theta|\alpha,\beta,y)
    &\propto \prod_k \theta_k^{y_k+\alpha_k - 1}\\
    &\implies \theta|\alpha,\beta,y \sim Dir(\vec{y} + \vec{\alpha})\\
  P(\alpha|\theta,\beta,y) 
    &\propto \mathcal{B}(\vec{\alpha})
      \prod_k \theta_k^{y_k+\alpha_k - 1}\alpha_k^{-(\beta + 1)}\\
    &\implies \text{unknown distribution}\\
  P(\beta|\theta,\alpha,y)
    &\propto \beta^{K-1} \gamma^{\beta k}(\prod_k\alpha_k)^{-(\beta + 1)}\\
    &\propto \beta^{K-1} \gamma^{\beta k}(\prod_k\alpha_k)^{-\beta}\\
    &\propto \beta^{K-1} \exp\Bigg[-\beta \bigg(\sum_k\log(\alpha_k)-k\log(\gamma)\bigg)\Bigg]\\
    &\implies \beta|\theta,\alpha,y \sim Gamma\Big(k, \sum_k\log(\alpha_k)-k\log(\gamma)\Big)
\end{align}

##Jeffrey's prior on $\beta$

\begin{align}
p(\beta)&\propto\prod_k\beta\alpha_k^{-(\beta + 1)}\\
p(\beta)&\propto\beta^k (\prod_k\alpha_k)^{-(\beta + 1)}\\
\log\big(p(\beta)\big)&\propto k\log(\beta)-\beta\log(\prod_k \alpha_k)-\log(\prod_k \alpha_k)\\
\frac{\partial}{\partial\beta}\log\big(p(\beta)\big)&\propto \frac{k}{\beta}-\log(\prod_k\alpha_k)\\
\frac{\partial^2}{\partial\beta^2}\log\big(p(\beta)\big)&\propto \frac{-k}{\beta^2}\\
-E\Big[\frac{\partial^2}{\partial\beta^2}\log\big(p(\beta)\big)\Big]&\propto \frac{k}{\beta^2}\\
\pi(\beta)&\propto \frac{1}{\beta}
\end{align}
























